{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание собственного скрейпера\n",
    "\n",
    "Библиотеки, которые нам понадобятся:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стандартные\n",
    "import re\n",
    "import urllib.request\n",
    "from time import sleep\n",
    "# нужно установить: pip install bs4\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядит заглавная страница ресурса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = urllib.request.urlopen('https://proza.ru/')  # откроем ее с помощью библиотеки urllib\n",
    "\n",
    "webpage = fid.read().decode('cp1251')  # как узнать, какая нам нужна кодировка: нужно открыть в браузере эту страницу, нажать f12 (для chrome) и в терминале (console) набрать document.characterSet\n",
    "\n",
    "print(webpage)  # посмотрим, как оно выглядит"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"console.PNG\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(webpage, 'html.parser')  # передадим нашу страницу в парсер bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    # посмотрим, какие у нас есть ссылки\n",
    "    print(link.get('href'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нас интересуют только внутренние, а они, очевидно, не начинаются с https. Отсеем их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for link in soup.find_all('a', attrs={'href': re.compile(r'^/.+')}):\n",
    "    links.append('https://proza.ru' + link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links[:10] # посмотрим, что вышло"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вроде похоже на правду. Давайте собирать уже просмотренные ссылки в множество, чтобы не попадать в них по нескольку раз:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewedlinks = set('https://proza.ru/') # главную мы типа уже посмотрели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем в тестовом режиме пройтись по ссылкам до упора один раз. Давайте оформим наше извлечение страницы в какую-нибудь функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looklinks(page, viewed):\n",
    "    \"\"\"Тестовая функция для просмотра всех ссылок на странице\"\"\"\n",
    "    fid = urllib.request.urlopen(page) \n",
    "    webpage = fid.read().decode('cp1251')\n",
    "    viewed.add(page)\n",
    "    soup = bs(webpage, 'html.parser') \n",
    "    links = []\n",
    "    for link in soup.find_all('a', attrs={'href': re.compile(r'^/.+')}):\n",
    "        l = 'https://proza.ru' + link.get('href')\n",
    "        if l not in viewed:\n",
    "            links.append(l)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = looklinks(links[0], viewedlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = looklinks(step[0], viewedlinks)\n",
    "step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если прощелкать несколько раз, рано или поздно дойдем до пустого списка: значит, ссылки на странице закончились. Давайте для проверки, что находится на страницах с авторами, прощелкаем более избирательно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'https://proza.ru/avtor/akochevnik'\n",
    "# 'https://proza.ru/avtor/akochevnik&book=9#9'\n",
    "# 'https://proza.ru/2012/11/30/1362'\n",
    "# 'https://proza.ru/complain.html?text_2012/11/30/1362'\n",
    "# 'https://proza.ru/about/pravila.html'\n",
    "\n",
    "step = looklinks('https://proza.ru/2012/11/30/1362', viewedlinks)\n",
    "step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну вроде как понятно: все должно работать. Нам теперь осталось понять, на страницах какого вида лежит хороший текст. Предположу, что на страницах с датами. Проверим, напринтив такую страницу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = urllib.request.urlopen('https://proza.ru/2012/11/30/1362') \n",
    "webpage = fid.read().decode('cp1251')\n",
    "print(webpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно: тут лежат наши тексты. Возможно, они еще где-нибудь лежат (poem?), это тоже можно проверить. Ну предположим, что мы все проверили и убедились, что страницы с текстами произведений - это, очевидно, адреса в формате r'\\d{4}/\\d\\d/\\d\\d/.*' = '1234/56/78/123...'\n",
    "\n",
    "Давайте попробуем допилить нашу тестовую функцию таким образом, чтобы она дополнительно проверяла, не является ли просматриваемая страница текстовой, и добавляла ее в отдельный список. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looklinks(page, viewed, texts):\n",
    "    \"\"\"Тестовая функция для просмотра всех ссылок на странице и добавления текстов\"\"\"\n",
    "    print(page)\n",
    "    fid = urllib.request.urlopen(page) \n",
    "    webpage = fid.read().decode('cp1251')\n",
    "    viewed.add(page)\n",
    "    if re.match(r'https://proza.ru/\\d{4}/\\d\\d/\\d\\d/.*', page):\n",
    "        texts.append(webpage)\n",
    "        print(f'text for {page} added!')\n",
    "    soup = bs(webpage, 'html.parser') \n",
    "    links = []\n",
    "    for link in soup.find_all('a', attrs={'href': re.compile(r'^/.+')}):\n",
    "        l = 'https://proza.ru' + link.get('href')\n",
    "        if l not in viewed:\n",
    "            links.append(l)\n",
    "    if links:\n",
    "        for link in links:\n",
    "            sleep(1)  # чтоб нас случайно не заблочили как робота\n",
    "            looklinks(link, viewed, texts)\n",
    "    else:\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потестим нашу функцию для какого-нибудь конкретного автора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 'https://proza.ru/avtor/evgeny1951mai'\n",
    "looklinks(start, viewedlinks, texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что все работает и что наш скрейпер, как ему и полагается, ползает со страницы на страницу и рано или поздно выкачает всю прозу, если мы не установим какой-нибудь лимит, например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looklinks(page, viewed, texts, N=10):\n",
    "    \"\"\"Тестовая функция для просмотра всех ссылок на странице и добавления текстов\"\"\"\n",
    "    if len(texts) > N:  # N - число текстов, которое мы хотим выкачать\n",
    "        return\n",
    "    print(page)\n",
    "    fid = urllib.request.urlopen(page) \n",
    "    webpage = fid.read().decode('cp1251')\n",
    "    viewed.add(page)\n",
    "    if re.match(r'https://proza.ru/\\d{4}/\\d\\d/\\d\\d/.*', page):\n",
    "        texts.append(webpage)\n",
    "        print(f'text for {page} added!')\n",
    "    soup = bs(webpage, 'html.parser') \n",
    "    links = []\n",
    "    for link in soup.find_all('a', attrs={'href': re.compile(r'^/.+')}):\n",
    "        l = 'https://proza.ru' + link.get('href')\n",
    "        if l not in viewed:\n",
    "            links.append(l)\n",
    "    if links:\n",
    "        for link in links:\n",
    "            sleep(1)  # чтоб нас случайно не заблочили как робота\n",
    "            looklinks(link, viewed, texts)\n",
    "    else:\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, дело за малым: вытащить собственно тексты из html-разметки :) И метаинформацию о них: заголовок, автора и дату публикации. \n",
    "\n",
    "Заголовок, очевидно, лежит в самом html, это:\n",
    "\n",
    "        <title>Моё деревенское счастье. Предисловие (Александр Алексеевич Кочевник) / Проза.ру</title> \n",
    "\n",
    "Автор тоже есть, он зашит в заголовке: r'(.+?) \\\\((.+?)\\\\) / Проза.ру'\n",
    "\n",
    "Ну а дату можно из ссылки страницы извлечь, мы уже выяснили, что эти ссылки нумеруются датами. Следовательно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looklinks(page, viewed, texts, N=10):\n",
    "    \"\"\"Тестовая функция для просмотра всех ссылок на странице и добавления текстов\"\"\"\n",
    "    print(page)\n",
    "    fid = urllib.request.urlopen(page) \n",
    "    webpage = fid.read().decode('cp1251')\n",
    "    viewed.add(page)\n",
    "    soup = bs(webpage, 'html.parser') \n",
    "    # вот наш кусочек, который будет отвечать за добавление всего в итоговый список текстов:\n",
    "    if re.match(r'https://proza.ru/\\d{4}/\\d\\d/\\d\\d/.*', page):\n",
    "        ta = re.search(r'(.+?) \\((.+?)\\) / Проза.ру', soup.title.string) # расчленяем заголовок на автора и название с помощью групп в регулярках\n",
    "        if ta:\n",
    "            title = ta.group(1)\n",
    "            author = ta.group(2)\n",
    "        else:\n",
    "            # а вдруг мы где-то накосячили с регуляркой\n",
    "            title = 'Unknown'\n",
    "            author = 'Unknown'\n",
    "        year = re.search(r'https://proza.ru/(\\d{4}/\\d\\d/\\d\\d)/.*', page).group(1) # получаем год с помощью группы\n",
    "        text = '\\n'.join((elem.text for elem in soup.find_all(\"div\", {\"class\": \"text\"})))  # собираем все возможные сегменты класса \"текст\" с помощью bs4\n",
    "        texts.append({'title': title, 'author': author, 'year': year, 'text': text})\n",
    "        print(f'text for {page} added!')\n",
    "    links = []\n",
    "    for link in soup.find_all('a', attrs={'href': re.compile(r'^/.+')}):\n",
    "        l = 'https://proza.ru' + link.get('href')\n",
    "        if l not in viewed:\n",
    "            links.append(l)\n",
    "    if links:\n",
    "        for link in links:\n",
    "            if re.search('board|login|help|topic|type|about', link): # мы, вероятно, не хотим бесконечно лазать по внутренностям не интересующих нас разделов\n",
    "                continue\n",
    "            if len(texts) >= N:  # N - число текстов, которое мы хотим выкачать\n",
    "                return\n",
    "            sleep(0.5)  # чтоб нас случайно не заблочили как робота\n",
    "            looklinks(link, viewed, texts, N)\n",
    "    else:\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewedlinks = set('https://proza.ru/') # для чистоты эксперимента обнулим наш список просмотренного\n",
    "texts = [] # и список текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 'https://proza.ru/avtor/evgeny1951mai' # начнем с того же дяденьки, но укажем, что нам нужно только 10 текстов: мы потестить\n",
    "looklinks(start, viewedlinks, texts, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну вот и все! осталось для красоты только переписать этот код на классах и в .py....... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bb79a41a93e0a2295d76faf94992bd34b2d2d698776e22ca1184eb4e5811e07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
